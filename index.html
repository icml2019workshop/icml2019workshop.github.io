<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="ICML 2019 Workshop on Security and Privacy of Machine Learning">

  <title>Security and Privacy of Machine Learning</title>

  <!-- Bootstrap core CSS -->
  <link href="bootstrap.min.css" rel="stylesheet">
</head>

<body>

<!-- Begin page content -->
<main role="main" class="container">
  <h1 class="mt-5">Security and Privacy of Machine Learning
</h1>
  <p class="mb-0"><b>Date:</b> June 14, 2019</p>
  <p class="mb-0"><b>Location:</b> Long Beach, CA, USA</p>
  <!-- <p class="mb-0"><b>Contact:</b> xxx (this will email all organizers)</p> -->
  <p>
    As machine learning has increasingly been deployed in critical real-world applications, the dangers of manipulation and misuse of these models has become of paramount importance to public safety and user privacy.  In applications such as online content recognition to financial analytics to autonomous vehicles all have shown the be vulnerable to adversaries wishing to manipulate the models or mislead models to their malicious ends.  
  </p>
  <p>
    This workshop will focus on recent research and future directions about the security and privacy problems in real-world machine learning systems. We aim to bring together experts from machine learning, security, and privacy communities in an attempt to highlight recent work in these area as well as to clarify the foundations of secure and private machine learning strategies. We seek to come to a consensus on a rigorous framework to formulate adversarial attacks targeting machine learning models, and to characterize the properties that ensure the security and privacy of machine learning systems. Finally, we hope to chart out important directions for future work and cross-community collaborations.
  </p>
  <!-- <h2>Sponsor</h2> -->
  <!-- <p></p> -->

  <h2>Schedule</h2>
  <!-- <p>The following is a tentative schedule and is subject to change prior to the workshop.</p> -->

  <table class="table table-sm">
    <tbody>
    <tr>
      <th scope="row">8:40am-9:00am</th>
      <td>Opening Remarks</td>
      <td></td>
    </tr>

    <tr><th scope="row" colspan="3">Session 1: Security Vulnerabilities of Machine Learning Systems</th></tr>
    <tr>
      <th scope="row">9:00am-9:30am</th>
      <td>Invited Talk #1: Patrick McDaniel </td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">9:30am-10:00am</th>
      <td>Inivted Talk #2: Una-May O'Reilly</td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">10:00am-10:20am</th>
      <td>Contributed Talk #1: Enhancing Gradient-based Attacks with Symbolic Intervals </td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">10:20am-10:30am</th>
      <td>Spotlight Presentation #1: Adversarial Policies: Attacking Deep Reinforcement Learning </td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">10:30am-10:45am</th>
      <td>Coffee Break </td>
    </tr>
    <tr><th scope="row" colspan="3">Session 2: Secure and Private Machine Learning in Practice</th></tr>
    <tr>
      <th scope="row">10:45am-11:15am</th>
      <td>Invited Talk #3: Le Song </td>
      <td></td>
    </tr>
    
    <tr>
      <th scope="row">11:15am-11:45am</th>
      <td>Invited Talk #4: Allen Qi </td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">11:45am-12:05pm</th>
      <td>Contributed Talk #2: Private vqSGD: Vector-Quantized Stochastic Gradient Descent </td>
      <td></td>
    </tr>
    <!-- <tr>
      <th scope="row">11:45am</th>
      <td>Spotlight Presentations #2: </td>
      <td></td>
    </tr> -->
    <tr>
      <th scope="row">12:05pm-1:15pm</th>
      <td>Lunch </td>
    </tr>
    <tr><th scope="row" colspan="3">Session 3: Provable Robustness and Verifiable Machine Learning Approaches</th></tr>
    
    <tr>
      <th scope="row">1:15pm-1:45pm</th>
      <td>Invited Talk #5: Ziko Kolter
 </td>
      <td><a target="_blank"></a></td>
    </tr>
    <tr>
      <th scope="row">1:45pm-2:05pm</th>
      <td>Contributed Talk #3: Provable Certificates for Adversarial Examples:Fitting a Ball in the Union of Polytopes</td>
      <td></td>
    </tr>


    <!-- <tr><th scope="row" colspan="3">2:00pm</th></tr> -->
    <tr>
      <th scope="row">2:05pm-2:45pm</th>
      <td>Poster Session #1</td>
      <td></td>
    </tr>

    <tr><th scope="row" colspan="3">Session 4:  Trustworthy and Interpretable Machine Learning Towards 
  </th></tr>
    <tr>
      <th scope="row">2:45pm-3:15pm</th>
      <td>Invited Talk #6: Alexander Madry </td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">3:15pm-3:45pm</th>
      <td>Invited Talk #7: Been Kim </td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">3:45pm-4:05pm</th>
      <td>Contributed Talk #4: Theoretically Principled Trade-off between Robustness and Accuracy </td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">4:05pm-4:15pm</th>
      <td>Spotlight Presentation #2: Model weight theft with just noise inputs: The curious case of the petulant attacker </td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">4:15pm-5:15pm</th>
      <td>Panel discussion </td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">5:15pm-6:00pm</th>
      <td>Poster Sesson #2</td>
      <td></td>
    </tr>
    </tbody>
  </table>
 <!--  
<h2>Important Dates</h2>
<ul>
  <li style="color: red;text-decoration-line: line-through;">Workshop paper submission deadline: 5/10/2019</li>
<li>Workshop paper submission deadline: 5/20/2019</li>
  <li>Notification to authors: 6/01/2019</li>
  <li>Camera ready deadline: 6/12/2019</li>
</ul> -->

    <h2>Schedule</h2>
    <p style="color: blue;"> Poster Session #1 (2:00pm-2:45pm)</p>
    <ul>
      <li>Shiqi Wang, Yizheng Chen, Ahmed Abdou and Suman Jana. Enhancing Gradient-based Attacks with Symbolic Intervals</li>
      <li>Bo Zhang, Boxiang Dong, Hui Wendy Wang and Hui Xiong. Integrity Verification for Federated Machine Learning in the Presence of Byzantine Faults</li>
      <li>Xinyun Chen, Wenxiao Wang, Yiming Ding, Chris Bender, Ruoxi Jia, Bo Li and Dawn Song. Leveraging Unlabeled Data for Watermark Removal of Deep Neural Networks</li>
      <li>Qian Lou and Lei Jiang. SHE: A Fast and Accurate Deep Neural Network for Encrypted Data</li>
      <li>Matt Jordan, Justin Lewis and Alexandros G. Dimakis. Provable Certificates for Adversarial Examples:Fitting a Ball in the Union of Polytopes</li>
      <li>Aria Rezaei, Chaowei Xiao, Bo Li and Jie Gao. Protecting Sensitive Attributes via Generative Adversarial Networks</li>
      <li>Saeed Mahloujifar, Mohammad Mahmoody and Ameer Mohammed. Universal Multi-Party Poisoning Attacks</li>
      <li>Hongge Chen, Huan Zhang, Si Si, Yang Li, Duane Boning and Cho-Jui Hsieh. Verifying the Robustness of Tree-based Models</li>
      <li>Congyue Deng and Yi Tian. Towards Understanding the Trade-off Between Accuracy and Adversarial Robustness</li>
      <li>Zhi Xu, Chengtao Li and Stefanie Jegelka. Exploring the Robustness of GANs to Internal Perturbations</li>
      <li>Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent Ghaoui and Michael Jordan. Theoretically Principled Trade-off between Robustness and Accuracy</li>
      <li>Pang Wei Koh, Jacob Steinhardt and Percy Liang. Stronger Data Poisoning Attacks Break Data Sanitization Defenses</li>
      <li>Bokun Wang and Ian Davidson. Improve Fairness of Deep Clustering to Prevent Misuse in Segregation</li>
      <li>Adam Gleave, Michael Dennis, Neel Kant, Cody Wild, Sergey Levine and Stuart Russell. Adversarial Policies: Attacking Deep Reinforcement Learning</li>
      <li>Yunhan Jia, Yantao Lu, Junjie Shen, Qi Alfred Chen, Zhenyu Zhong and Tao Wei. Attacking Multiple Object Tracking using Adversarial Examples</li>
      <li>Joseph Szurley and Zico Kolter. Perceptual Based Adversarial Audio Attacks</li>
    </ul>
    <p style="color: blue;">Poster Session #2 (5:15pm-6:00pm)</p>
    <ul> 
    <li>Felix Michels, Tobias Uelwer, Eric Upschulte and Stefan Harmeling. On the Vulnerability of Capsule Networks to Adversarial Attacks</li>
    <li>Zhaoyang Lyu, Ching-Yun Ko, Tsui-Wei Weng, Luca Daniel, Ngai Wong and Dahua Lin. POPQORN: Quantifying Robustness of Recurrent Neural Networks</li>
    <li>Avishek Ghosh, Justin Hong, Dong Yin and Kannan Ramchandran. Robust Heterogeneous Federated Learning</li>
    <li>Ruoxi Jia, Bo Li, Chaowei Xiao and Dawn Song. Delving into Bootstrapping for Differential Privacy</li>
    <li>Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu and Bin Dong. You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle</li>
    <li>Mark Lee and Zico Kolter. On Physical Adversarial Patches for Object Detection</li>
    <li>Venkata Gandikota, Raj Kumar Maity and Arya Mazumdar. Private vqSGD: Vector-Quantized Stochastic Gradient Descent</li>
    <li>Dimitrios Diochnos, Saeed Mahloujifar and Mohammad Mahmoody. Lower Bounds for Adversarially Robust PAC Learning</li>
    <li>Nicholas Roberts and Vinay Prabhu. Model weight theft with just noise inputs: The curious case of the petulant attacker</li>
    <li>Ryan Webster, Julien Rabin, Frederic Jurie and Loic Simon. Generating Private Data Surrogates for Vision Related Tasks</li>
    <li>Joyce Xu, Dian Ang Yap and Vinay Prabhu. Understanding Adversarial Robustness Through Loss Landscape Geometries</li>
    <li>Kevin Shi, Daniel Hsu and Allison Bishop. A cryptographic approach to black-box adversarial machine learning</li>
    <li>Haizhong Zheng, Earlence Fernandes and Atul Prakash. Analyzing the Interpretability Robustness of Self-Explaining Models</li>
    <li>Fatemehsadat Mireshghallah, Mohammadkazem Taram, Prakash Ramrakhyani, Sicun Gao, Dean Tullsen and Hadi Esmaeilzadeh. Shredder: Learning Noise for Privacy with Partial DNN Inference on the Edge</li>
    <li>Chaowei Xiao, Xinlei Pan, Warren He, Bo Li, Jian Peng, Mingjie Sun, Jinfeng Yi, Mingyan Liu, Dawn Song. Characterizing Attacks on Deep Reinforcement Learning</li>
    <li>Sanjam Garg, Somesh Jha, Saeed Mahloujifar and Mohammad Mahmoody. Adversarially Robust Learning Could Leverage Computational Hardness</li>
    </ul>

  
  


  <h2>Organizing Committee</h2>
  <p>(Listed by alphabetical order)</p>
  <div class="row justify-content-around">
    <!-- <div class="col-lg-1"></div> -->
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/dan.jpeg" width="100px" height="100px">
      <p style="width:100px" ><a href="https://crypto.stanford.edu/~dabo/">Dan Boneh</a></p>
    </div>
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/david.jpg" width="100px" height="100px">
      <p style="width:100px" ><a href="http://www.cs.virginia.edu/~evans/">David Evans</a></p>
    </div>
  <div class="col-md-1">
      <img class="rounded-circle" src="imgs/jha.jpg" width="100px" height="100px">
    <p style="width:100px" ><a href="http://pages.cs.wisc.edu/~jha/">Somesh Jha</a></p>
    </div>
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/boli.jpg" width="100px" height="100px">
      <p style=""width:100px;text-align: center;"" ><a href="http://www.crystal-boli.com"> Bo Li </a></p>
    </div>
     <div class="col-md-1">
      <img class="rounded-circle" src="imgs/liang.jpg" width="100px" height="100px">
       <p style="width:100px" ><a href="https://cs.stanford.edu/~pliang/">Percy Liang</a></p>
    </div>

     </div>
    <div class="row justify-content-around">
     
   
   <div class="col-md-1">
      <img class="rounded-circle" src="imgs/patrick.png" width="100px" height="100px">
     <p style="width:150px" ><a href="http://patrickmcdaniel.org"> Patrick McDaniel</a></p>
    </div>

    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/nicolas.jpg" width="100px" height="100px">
      <p style="width:140px" ><a href="https://www.papernot.fr">Nicolas Papernot<br /></a> </p>
    </div>
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/jocob.png" width="100px" height="100px">
      <p style="width:150px" ><a href="https://cs.stanford.edu/~jsteinhardt/">Jacob Steinhardt</a></p>
    </div>
     <div class="col-md-1">
      <img class="rounded-circle" src="imgs/dawn.png" width="100px" height="100px">
       <p style="width:100px" ><a href="https://people.eecs.berkeley.edu/~dawnsong/">Dawn Song</a></p>
    </div>

    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/florian.jpg" width="100px" height="100px">
      <p style="width:120px" ><a href="https://floriantramer.com">Florian Tramer</a></p>
    </div>
    
    <!-- </div> -->
    
 
  </div>
    <!-- <div class="col-lg-1"></div> -->
  </div>

<h2>Program Committee</h2>
<li>Chaowei Xiao (University of Michigan)</li>
<li>Xinyun Chen (University of California, Berkeley)</li>
<li>Mingjie Sun (Tsinghua University)</li>
<li>Linyi Li (University of Illinois at Urbana-Champaign)</li>
<li>Ruoxi Jia (University of California, Berkeley)</li>
<li>Kimin Lee (Korea Advanced Institute of Science and Technology)</li>
<li>Yunhan Jia (Baidu X-Lab)</li>
<li>Dimitris Tsipras (Massachusetts Institute of Technology)</li>
<li>Qi Alfred Chen (University of California, Irvine)</li>
<li>Huan Zhang (University of California, Los Angeles)</li>
<li>Yizheng Chen (Georgia Institute of Technology)</li>
<li>Hadi Abdullah (UF-FICS)</li>
<li>Octavian Suciu (University of Maryland)</li>
<li>Sixie Yu (Washington University in St. Louis)</li>
<li>Jonathan Uesato (Deepmind)</li>
<li>Liang Tong (Vanderbilt University)</li>
<li>Qiuyuan Huang (Microsoft)</li>
<li>Yigitcan Kaya (University of Maryland)</li>
<li>Kaizhao Liang (University of Illinois, Urbana Champaign)</li>
<li>Matthew Wicker (University of Georgia)</li>
<li>Anand Bhattad (University of Illinois at Urbana-Champaign)</li>
<li>Xinchen Yan(University of Michigan, Ann Arbor)</li>
<li>Karl Ni (Google LLC)</li>
<li>Huichen Li (Shanghai Jiao Tong University)</li>
<li>Li Erran Li (Pony.ai and Columbia University)</li>
<li>Lin Zhuo (Shanghai Jiao Tong Univerisity)</li>
<li>Hadi Salman (Microsoft Research AI)</li>
<li>Pengchuan Zhang (Microsoft)</li>
<li>Jerry Li (Microsoft)</li>
<li>Chen Qian (tencent)</li>
<li>Weilin Xu (University of Virginia)</li>
<li>Warren He (University of California, Berkeley)</li>
<li>Varun Chandrasekaran (University of Wisconsin-Madison)</li>
<li>Yuxin Wu (Tsinghua University)</li>
<li>Xingxing Wei (Tsinghua University)</li>
<li>Kathrin Grosse (CISPA, Saarland University)</li>
<li>Min Jin Chong (University of Illinois at Urbana-Champaign)</li>
<li>Chao Yan (Chinese Academy of Sciences)</li>
<li>Shreya Shankar (Stanford University)</li>
<li>Eric Wong (Carnegie Mellon University)</li>
<li>Mantas Mazeika (University of Chicago)</li>
<li>Fartash Faghri (University of Toronto)</li>
<li>Pin-Yu Chen (IBM)</li>
<li>Yulong Cao (University of Michigan)</li>
<li>Xiaowei Huang (University of Liverpool)</li>
<li>Hongge Chen (Massachusetts Institute of Technology)</li>
<li>Greg Yang (Microsoft)</li>
<li>Hamid Palangi (Microsoft)</li>
<!-- <li>Bhavya Khailkhura (Lawrence Livermore National Lab)</li>
<li>Catherine Olsson (Google Brain)</li>
<li>Chaowei Xiao (University of Michigan)</li>
<li>David Evans (University of Virginia)</li>
<li>Dimitris Tsipras (Massachusetts Institute of Technology)</li>
<li>Earlence Fernandes (University of Washington)</li>
<li>Eric Wong (Carnegie Mellon University)</li>
<li>Fartash Faghri (University of Toronto)</li>
<li>Florian Tramer (Stanford University)</li>
<li>Hadi Abdullah (University of Florida)</li>
<li>Hao Su (UCSD)</li>
<li>Jonathan Uesato (DeepMind)</li>
<li>Karl Ni (In-Q-Tel)</li>
<li>Kassem Fawaz (University of Wisconsin-Madison)</li>
<li>Kathrin Grosse (CISPA)</li>
<li>Krishna Gummadi (MPI-SWS)</li>
  <li> Li Erran Li (Pony.AI)</li>
<li>Matthew Wicker (University of Georgia)</li>
<li>Nathan Mundhenk (Lawrence Livermore National Lab)</li>
<li>Nicholas Carlini (Google Brain)</li>
<li>Nicolas Papernot (Google Brain and University of Toronto)</li>
<li>Octavian Suciu (University of Maryland)</li>
<li>Pin-Yu Chen (IBM)</li>
<li>Pushmeet Kohli (DeepMind)</li>
  <li>Qian Chen (Tencent) </li>
<li>Shreya Shankar (Stanford University)</li>
<li>Suman Jana (Columbia University)</li>
<li>Varun Chandrasekaran (University of Wisconsin-Madison)</li>
<li>Xiaowei Huang (Liverpool University)</li>
<li>Yanjun Qi (University of Virginia)</li>
<li>Yigitcan Kaya (University of Maryland)</li>
<li>Yizheng Chen (Georgia Tech)</li>
 -->







<h2>Call For Papers</h2>
  <p class="mb-0" ><b>Submission deadline:</b> May 20, 2019 Anywhere on Earth (AoE)</p>
  <p class="mb-0"><b>Notification sent to authors:</b> June 3, 2019 Anywhere on Earth (AoE)</p>
  <p class="mb-0"><b>Submission server:</b> <a href="https://easychair.org/conferences/?conf=spml19" target="_blank">https://easychair.org/conferences/?conf=spml19</a></p>
  <p>Submissions to this track will introduce novel ideas or results. Submissions should follow the ICML format and not exceed 4 pages (excluding references, appendices or large figures).</p>
  
  <p>The workshop will include contributed papers. Based on the PC’s recommendation, each paper accepted to the workshop will be allocated either a contributed talk or poster presentation .</p>
  <p>Submissions need to be anonymized. The workshop allows submissions of papers that are under review or have been recently published in a conference or a journal. The workshop will not have any official proceedings.</p>
  <p>We invite submissions on <b>any aspect of machine learning that relates to computer security and privacy (and vice versa)</b>. This includes, but is not limited to:</p>

  <ul>
    <li> Test-time (exploratory) attacks: e.g. adversarial examples
    </li>
<li>Training-time (causative) attacks: e.g. data poisoning attack</li>
<li>Differential privacy</li>
<li>Privacy preserving generative models</li>
<li>Game theoretic analysis on machine learning models</li>
<li>Manipulation of crowd-sourcing systems</li>
<li>Sybil detection</li>
<li>Exploitable bugs in ML systems</li>
<li>Formal verification of ML systems</li>
<li>Model stealing</li>
<li>Misuse of AI and deep learning</li>
<li>Interpretable machine learning</li>
  </ul>








</body></html>
